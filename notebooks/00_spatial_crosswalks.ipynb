{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c8cbd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG ROOT: /Users/warrenjones/Dev/igs-analysis-baltimore\n"
     ]
    }
   ],
   "source": [
    "# Make project root importable and (re)load config cleanly\n",
    "from pathlib import Path\n",
    "import sys, importlib\n",
    "\n",
    "NB_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NB_DIR.parent if NB_DIR.name == \"notebooks\" else NB_DIR\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import config\n",
    "importlib.reload(config)  # ensure we see the latest edits\n",
    "\n",
    "from config import ROOT, RAW_SHAPES, PROJ_SHAPES, TRACTS_RAW, CSAS_RAW, TRACTS_FP, CSAS_FP\n",
    "print(\"CONFIG ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e247251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG ROOT: /Users/warrenjones/Dev/igs-analysis-baltimore\n"
     ]
    }
   ],
   "source": [
    "# make project root importable\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "NB_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NB_DIR.parent if NB_DIR.name == \"notebooks\" else NB_DIR\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config import ROOT, RAW_SHAPES, PROJ_SHAPES, TRACTS_RAW, CSAS_RAW, TRACTS_FP, CSAS_FP\n",
    "\n",
    "print(\"CONFIG ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfcaad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ROOT, RAW_SHAPES, PROJ_SHAPES, TRACTS_RAW, CSAS_RAW, TRACTS_FP, CSAS_FP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b5c01d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:\n",
      "   ../data_raw/shapes/2020_Census_Tracts_(Census_TIGER).geojson\n",
      "   ../data_raw/shapes/Community_Statistical_Areas_(CSAs)__Reference_Boundaries.geojson\n",
      "Loaded tracts: 717\n",
      "Loaded CSAs:   56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>STATEFP20</th>\n",
       "      <th>COUNTYFP20</th>\n",
       "      <th>TRACTCE20</th>\n",
       "      <th>GEOID20</th>\n",
       "      <th>NAME20</th>\n",
       "      <th>NAMELSAD20</th>\n",
       "      <th>MTFCC20</th>\n",
       "      <th>FUNCSTAT20</th>\n",
       "      <th>ALAND20</th>\n",
       "      <th>AWATER20</th>\n",
       "      <th>INTPTLAT20</th>\n",
       "      <th>INTPTLON20</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>003</td>\n",
       "      <td>731103</td>\n",
       "      <td>24003731103</td>\n",
       "      <td>7311.03</td>\n",
       "      <td>Census Tract</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>8562084</td>\n",
       "      <td>0</td>\n",
       "      <td>+39.0343178</td>\n",
       "      <td>-076.4946333</td>\n",
       "      <td>51684.935897</td>\n",
       "      <td>9.215257e+07</td>\n",
       "      <td>POLYGON ((-76.52241 39.05171, -76.52232 39.051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>003</td>\n",
       "      <td>707001</td>\n",
       "      <td>24003707001</td>\n",
       "      <td>7070.01</td>\n",
       "      <td>Census Tract</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>27326340</td>\n",
       "      <td>9371758</td>\n",
       "      <td>+38.8202066</td>\n",
       "      <td>-076.5292506</td>\n",
       "      <td>108449.152983</td>\n",
       "      <td>3.949741e+08</td>\n",
       "      <td>POLYGON ((-76.58707 38.82953, -76.58707 38.829...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID STATEFP20 COUNTYFP20 TRACTCE20      GEOID20   NAME20  \\\n",
       "0         1        24        003    731103  24003731103  7311.03   \n",
       "1         2        24        003    707001  24003707001  7070.01   \n",
       "\n",
       "     NAMELSAD20 MTFCC20 FUNCSTAT20   ALAND20  AWATER20   INTPTLAT20  \\\n",
       "0  Census Tract   G5020          S   8562084         0  +39.0343178   \n",
       "1  Census Tract   G5020          S  27326340   9371758  +38.8202066   \n",
       "\n",
       "     INTPTLON20   Shape_Length    Shape_Area  \\\n",
       "0  -076.4946333   51684.935897  9.215257e+07   \n",
       "1  -076.5292506  108449.152983  3.949741e+08   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-76.52241 39.05171, -76.52232 39.051...  \n",
       "1  POLYGON ((-76.58707 38.82953, -76.58707 38.829...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- define paths explicitly so we don’t rely on earlier cells ---\n",
    "ROOT = Path(\"..\") if (Path.cwd().name == \"notebooks\") else Path(\".\")\n",
    "RAW_SHAPES = ROOT / \"data_raw\" / \"shapes\"\n",
    "\n",
    "# raw TIGER and CSA reference files\n",
    "TRACTS_RAW = RAW_SHAPES / \"2020_Census_Tracts_(Census_TIGER).geojson\"\n",
    "CSAS_RAW   = RAW_SHAPES / \"Community_Statistical_Areas_(CSAs)__Reference_Boundaries.geojson\"\n",
    "\n",
    "# --- load files ---\n",
    "print(\"Loading:\")\n",
    "print(\"  \", TRACTS_RAW)\n",
    "print(\"  \", CSAS_RAW)\n",
    "\n",
    "metro = gpd.read_file(TRACTS_RAW)\n",
    "csas  = gpd.read_file(CSAS_RAW)\n",
    "\n",
    "print(f\"Loaded tracts: {len(metro):,}\")\n",
    "print(f\"Loaded CSAs:   {len(csas):,}\")\n",
    "metro.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4edeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting files:\n",
      "   ../data_raw/shapes/2020_Census_Tracts_(Census_TIGER).geojson\n",
      "   ../data_raw/shapes/Community_Statistical_Areas_(CSAs)__Reference_Boundaries.geojson\n",
      "\n",
      "Tracts columns: ['OBJECTID', 'STATEFP20', 'COUNTYFP20', 'TRACTCE20', 'GEOID20', 'NAME20', 'NAMELSAD20', 'MTFCC20', 'FUNCSTAT20', 'ALAND20', 'AWATER20', 'INTPTLAT20', 'INTPTLON20', 'Shape_Length', 'Shape_Area', 'geometry']\n",
      "CSAs columns   : ['FID', 'Community', 'Neigh', 'Link', 'CSA2020', 'geometry']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>STATEFP20</th>\n",
       "      <th>COUNTYFP20</th>\n",
       "      <th>TRACTCE20</th>\n",
       "      <th>GEOID20</th>\n",
       "      <th>NAME20</th>\n",
       "      <th>NAMELSAD20</th>\n",
       "      <th>MTFCC20</th>\n",
       "      <th>FUNCSTAT20</th>\n",
       "      <th>ALAND20</th>\n",
       "      <th>AWATER20</th>\n",
       "      <th>INTPTLAT20</th>\n",
       "      <th>INTPTLON20</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>003</td>\n",
       "      <td>731103</td>\n",
       "      <td>24003731103</td>\n",
       "      <td>7311.03</td>\n",
       "      <td>Census Tract</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>8562084</td>\n",
       "      <td>0</td>\n",
       "      <td>+39.0343178</td>\n",
       "      <td>-076.4946333</td>\n",
       "      <td>51684.935897</td>\n",
       "      <td>9.215257e+07</td>\n",
       "      <td>POLYGON ((-76.52241 39.05171, -76.52232 39.051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>003</td>\n",
       "      <td>707001</td>\n",
       "      <td>24003707001</td>\n",
       "      <td>7070.01</td>\n",
       "      <td>Census Tract</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>27326340</td>\n",
       "      <td>9371758</td>\n",
       "      <td>+38.8202066</td>\n",
       "      <td>-076.5292506</td>\n",
       "      <td>108449.152983</td>\n",
       "      <td>3.949741e+08</td>\n",
       "      <td>POLYGON ((-76.58707 38.82953, -76.58707 38.829...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID STATEFP20 COUNTYFP20 TRACTCE20      GEOID20   NAME20  \\\n",
       "0         1        24        003    731103  24003731103  7311.03   \n",
       "1         2        24        003    707001  24003707001  7070.01   \n",
       "\n",
       "     NAMELSAD20 MTFCC20 FUNCSTAT20   ALAND20  AWATER20   INTPTLAT20  \\\n",
       "0  Census Tract   G5020          S   8562084         0  +39.0343178   \n",
       "1  Census Tract   G5020          S  27326340   9371758  +38.8202066   \n",
       "\n",
       "     INTPTLON20   Shape_Length    Shape_Area  \\\n",
       "0  -076.4946333   51684.935897  9.215257e+07   \n",
       "1  -076.5292506  108449.152983  3.949741e+08   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-76.52241 39.05171, -76.52232 39.051...  \n",
       "1  POLYGON ((-76.58707 38.82953, -76.58707 38.829...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT        = Path(\"..\")                                # notebook is in /notebooks\n",
    "RAW_SHAPES  = ROOT / \"data_raw\" / \"shapes\"\n",
    "PROJ_SHAPES = ROOT / \"shapes\"\n",
    "PROJ_SHAPES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRACTS_RAW = RAW_SHAPES / \"2020_Census_Tracts_(Census_TIGER).geojson\"\n",
    "CSAS_RAW   = RAW_SHAPES / \"Community_Statistical_Areas_(CSAs)__Reference_Boundaries.geojson\"\n",
    "\n",
    "print(\"Expecting files:\")\n",
    "print(\"  \", TRACTS_RAW)\n",
    "print(\"  \", CSAS_RAW)\n",
    "\n",
    "assert TRACTS_RAW.exists(), f\"Missing: {TRACTS_RAW}\"\n",
    "assert CSAS_RAW.exists(),   f\"Missing: {CSAS_RAW}\"\n",
    "\n",
    "metro = gpd.read_file(TRACTS_RAW)\n",
    "csas  = gpd.read_file(CSAS_RAW)\n",
    "\n",
    "print(\"\\nTracts columns:\", list(metro.columns))\n",
    "print(\"CSAs columns   :\", list(csas.columns))\n",
    "metro.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50c7cd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/warrenjones/Dev/igs-analysis-baltimore/notebooks\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & paths ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# If you have pyogrio, GeoPandas reads much faster; otherwise it's fine.\n",
    "# import pyogrio  # optional\n",
    "\n",
    "# Project root (adjust if your notebook lives under /notebooks)\n",
    "ROOT = Path.cwd()\n",
    "DATA_RAW = ROOT / \"data_raw\"\n",
    "SHAPES = DATA_RAW / \"shapes\"\n",
    "OUT = ROOT / \"data_intermediate\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7c1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points_any(fp):\n",
    "    \"\"\"\n",
    "    Load point data from GeoJSON/GeoPackage/Shapefile and ensure a valid CRS.\n",
    "    Returns a GeoDataFrame in EPSG:4326 by default.\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(fp)\n",
    "    if gdf.empty:\n",
    "        raise ValueError(f\"No features found in {fp}\")\n",
    "    if gdf.crs is None:\n",
    "        # If source CRS is missing, set it if you actually know it; defaulting to 4326 is common for GeoJSON.\n",
    "        gdf.set_crs(4326, inplace=True)\n",
    "    # Normalize to WGS84 for consistency\n",
    "    gdf = gdf.to_crs(4326)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def load_polygons_any(fp, target_epsg=3857):\n",
    "    \"\"\"\n",
    "    Load polygon data (e.g., Census tracts). Reprojects to a metric CRS for spatial ops.\n",
    "    EPSG:3857 is fine for counts; use a local CRS if you need area-accurate calcs.\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(fp)\n",
    "    if gdf.empty:\n",
    "        raise ValueError(f\"No features found in {fp}\")\n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(4326, inplace=True)\n",
    "    return gdf.to_crs(target_epsg)\n",
    "\n",
    "\n",
    "def to_metric(gdf, target_epsg=3857):\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(4326)\n",
    "    return gdf.to_crs(target_epsg)\n",
    "\n",
    "\n",
    "def points_to_tract_counts(points_gdf, tracts_gdf, tract_id_col=\"GEOID\", count_col=\"points_count\"):\n",
    "    \"\"\"\n",
    "    Spatially join points to tracts and return a tract-level count GeoDataFrame.\n",
    "    points_gdf: GeoDataFrame with Point geometries\n",
    "    tracts_gdf: GeoDataFrame with tract polygons (metric CRS recommended)\n",
    "    \"\"\"\n",
    "    # Reproject points to match tracts for spatial join\n",
    "    pts = points_gdf\n",
    "    if pts.crs is None:\n",
    "        pts = pts.set_crs(4326)\n",
    "    pts = pts.to_crs(tracts_gdf.crs)\n",
    "\n",
    "    joined = gpd.sjoin(pts, tracts_gdf[[tract_id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    counts = (joined\n",
    "              .groupby(tract_id_col, dropna=False)\n",
    "              .size()\n",
    "              .rename(count_col)\n",
    "              .reset_index())\n",
    "\n",
    "    out = tracts_gdf.merge(counts, on=tract_id_col, how=\"left\")\n",
    "    out[count_col] = out[count_col].fillna(0).astype(int)\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_feat(gdf, name_no_ext, out_dir=OUT):\n",
    "    \"\"\"\n",
    "    Save both Parquet (attributes) and GeoParquet (geometry).\n",
    "    Creates <name>.parquet (GeoParquet if available) under data_intermediate by default.\n",
    "    \"\"\"\n",
    "    out_fp = out_dir / f\"{name_no_ext}.parquet\"\n",
    "    # If geopandas >=0.13 with pyarrow supports GeoParquet:\n",
    "    try:\n",
    "        gdf.to_parquet(out_fp)\n",
    "    except Exception:\n",
    "        # Fallback: drop geometry to plain Parquet and also write a GeoJSON for the map\n",
    "        tbl_fp = out_dir / f\"{name_no_ext}_table.parquet\"\n",
    "        gj_fp = out_dir / f\"{name_no_ext}.geojson\"\n",
    "        gdf.drop(columns=\"geometry\").to_parquet(tbl_fp)\n",
    "        gdf.to_file(gj_fp, driver=\"GeoJSON\")\n",
    "        return str(tbl_fp), str(gj_fp)\n",
    "    return str(out_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75d6924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 199 tracts  → ../shapes/baltimore_tracts_2020.geojson\n",
      "(CSA path reserved)          → ../shapes/baltimore_csa.geojson\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "# ---- Ensure paths exist even after a restart ----\n",
    "ROOT = Path(\"..\") if Path.cwd().name == \"notebooks\" else Path(\".\")\n",
    "PROJ_SHAPES = ROOT / \"shapes\"\n",
    "PROJ_SHAPES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Ensure 'metro' exists (load raw tracts if needed) ----\n",
    "try:\n",
    "    metro  # defined earlier?\n",
    "except NameError:\n",
    "    RAW_SHAPES = ROOT / \"data_raw\" / \"shapes\"\n",
    "    TRACTS_RAW = RAW_SHAPES / \"2020_Census_Tracts_(Census_TIGER).geojson\"\n",
    "    metro = gpd.read_file(TRACTS_RAW)\n",
    "\n",
    "# ---- Normalize column names, preserve geometry ----\n",
    "geom_col = metro.geometry.name  # usually 'geometry'\n",
    "metro_up = metro.rename(columns=str.upper)\n",
    "if \"GEOMETRY\" in metro_up.columns and \"geometry\" not in metro_up.columns:\n",
    "    metro_up = metro_up.rename(columns={\"GEOMETRY\": \"geometry\"})\n",
    "\n",
    "# ---- Choose TIGER fields with fallbacks ----\n",
    "STATE = \"STATEFP\"   if \"STATEFP\"   in metro_up.columns else \"STATEFP20\"\n",
    "COUNT = \"COUNTYFP\"  if \"COUNTYFP\"  in metro_up.columns else \"COUNTYFP20\"\n",
    "GEOID = \"GEOID\"     if \"GEOID\"     in metro_up.columns else (\n",
    "        \"GEOID20\" if \"GEOID20\" in metro_up.columns else \"GEOID10\"\n",
    ")\n",
    "\n",
    "# ---- Filter Maryland (24) -> Baltimore City (510) ----\n",
    "bal_city = metro_up.query(f\"{STATE} == '24' and {COUNT} == '510'\").copy()\n",
    "bal_city[\"tract_id\"] = bal_city[GEOID].astype(str)\n",
    "\n",
    "# ---- Keep only id + geometry, preserve CRS ----\n",
    "bal_city = gpd.GeoDataFrame(\n",
    "    bal_city[[\"tract_id\", \"geometry\"]],\n",
    "    geometry=\"geometry\",\n",
    "    crs=metro_up.crs\n",
    ")\n",
    "\n",
    "# ---- Save standardized project copies ----\n",
    "TRACTS_FP = PROJ_SHAPES / \"baltimore_tracts_2020.geojson\"\n",
    "CSAS_FP   = PROJ_SHAPES / \"baltimore_csa.geojson\"  # you’ll overwrite this later from your CSA cell\n",
    "bal_city.to_file(TRACTS_FP, driver=\"GeoJSON\")\n",
    "\n",
    "print(f\"Saved {len(bal_city)} tracts  → {TRACTS_FP}\")\n",
    "print(f\"(CSA path reserved)          → {CSAS_FP}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d4f817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    199.0\n",
       "mean       1.0\n",
       "std        0.0\n",
       "min        1.0\n",
       "25%        1.0\n",
       "50%        1.0\n",
       "75%        1.0\n",
       "max        1.0\n",
       "Name: weight_area, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "tracts = gpd.read_file(TRACTS_FP)  # has 'tract_id'\n",
    "csas   = gpd.read_file(CSAS_FP)\n",
    "\n",
    "# pick a CSA id column that exists\n",
    "if \"CSA2010\" in csas.columns:\n",
    "    csa_id_col = \"CSA2010\"\n",
    "elif \"CSA_NAME\" in csas.columns:\n",
    "    csa_id_col = \"CSA_NAME\"\n",
    "elif \"NAME\" in csas.columns:\n",
    "    csa_id_col = \"NAME\"\n",
    "else:\n",
    "    # last resort: first non-geometry column\n",
    "    csa_id_col = next(c for c in csas.columns if c != csas.geometry.name)\n",
    "\n",
    "csas = csas.rename(columns={csa_id_col: \"csa_id\"})[[\"csa_id\", \"geometry\"]]\n",
    "\n",
    "# project to a metric CRS for reliable area math\n",
    "proj = tracts.estimate_utm_crs()\n",
    "tracts_p = tracts.to_crs(proj)\n",
    "csas_p   = csas.to_crs(proj)\n",
    "\n",
    "# intersection + weights\n",
    "cross = gpd.overlay(tracts_p, csas_p, how=\"intersection\")\n",
    "cross[\"area_overlap\"] = cross.geometry.area\n",
    "\n",
    "tract_area = cross.groupby(\"tract_id\")[\"area_overlap\"].sum().rename(\"tract_area\")\n",
    "cross = cross.merge(tract_area, on=\"tract_id\", how=\"left\")\n",
    "cross[\"weight_area\"] = (cross[\"area_overlap\"] / cross[\"tract_area\"]).clip(0, 1)\n",
    "\n",
    "crosswalk = cross[[\"tract_id\", \"csa_id\", \"weight_area\"]].sort_values([\"tract_id\",\"csa_id\"])\n",
    "\n",
    "# QA: per-tract weights should sum ~ 1.0\n",
    "qa = crosswalk.groupby(\"tract_id\")[\"weight_area\"].sum().round(6).describe()\n",
    "qa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2041f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved crosswalk → ../data_clean/spatial_crosswalks/csa_to_tract.parquet | rows: 595\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUT = ROOT / \"data_clean\" / \"spatial_crosswalks\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "CW_FP = OUT / \"csa_to_tract.parquet\"\n",
    "crosswalk.to_parquet(CW_FP, index=False)\n",
    "print(\"Saved crosswalk →\", CW_FP, \"| rows:\", len(crosswalk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21449f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CW_FP = ROOT / \"data_clean/spatial_crosswalks\" / \"csa_to_tract.parquet\"\n",
    "CLEAN_COMM = ROOT / \"data_clean\" / \"community\"\n",
    "CLEAN_COMM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cw = pd.read_parquet(CW_FP)\n",
    "\n",
    "def _pick_csa_key(df):\n",
    "    for k in (\"CSA2010\",\"CSA_NAME\",\"NAME\",\"CSA_ID\"):\n",
    "        if k in df.columns:\n",
    "            return k\n",
    "    # last resort: first non-numeric column that isn't geometry-ish\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object and c.lower() not in (\"shape_area\",\"shape_length\",\"geometry\"):\n",
    "            return c\n",
    "    raise ValueError(\"Could not find a CSA key column.\")\n",
    "\n",
    "def _ensure_numeric(df, col):\n",
    "    # coerce with commas/percent signs handled\n",
    "    return pd.to_numeric(df[col].astype(str).str.replace(\",\",\"\").str.replace(\"%\",\"\"), errors=\"coerce\")\n",
    "\n",
    "def save_parquet(df, filename):\n",
    "    out = CLEAN_COMM / filename\n",
    "    df.to_parquet(out, index=False)\n",
    "    print(\"saved →\", out, \"| rows:\", len(df))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "522c77d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ../data_raw/community/Percent_Population_(25_Years_and_over)_With_High_School_Diploma_and_Some_College_or_Associates_Degree.csv\n",
      "CSA key: CSA2010 | value column: hsdipl11\n",
      "saved → ../data_clean/community/tract_pct_hs_or_more.parquet | rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tract_id</th>\n",
       "      <th>pct_hs_or_more_areaweighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tract_id, pct_hs_or_more_areaweighted]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 (replace): Downscale a CSA %/rate to tracts, robust\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "COMM_DIR = ROOT / \"data_raw\" / \"community\"\n",
    "\n",
    "# 1) pick the exact CSV you want (or let pattern find it)\n",
    "# Pattern looks for \"High_School\" & \"Some_College\"\n",
    "pattern = re.compile(r\"High[_ ]School.*Some[_ ]College\", re.I)\n",
    "candidates = [p.name for p in COMM_DIR.glob(\"*.csv\")]\n",
    "match = next((f for f in candidates if pattern.search(f)), None)\n",
    "if not match:\n",
    "    raise FileNotFoundError(\"Could not find the HS/Some College file. Pick one from:\\n\" + \"\\n\".join(candidates))\n",
    "\n",
    "INPUT_CSV = COMM_DIR / match\n",
    "print(\"Using:\", INPUT_CSV)\n",
    "\n",
    "# 2) load + clean columns\n",
    "csa = pd.read_csv(INPUT_CSV)\n",
    "csa.columns = [c.strip() for c in csa.columns]\n",
    "\n",
    "# 3) choose CSA key and a real percent/rate column (exclude IDs/geometry)\n",
    "def pick_csa_key(df):\n",
    "    for k in (\"CSA2010\",\"CSA_NAME\",\"NAME\",\"CSA_ID\"):\n",
    "        if k in df.columns:\n",
    "            return k\n",
    "    # fallback: first text column that isn't geometry-ish\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object and c.lower() not in (\"shape_area\",\"shape_length\",\"geometry\",\"objectid\"):\n",
    "            return c\n",
    "    raise ValueError(\"No CSA key found.\")\n",
    "\n",
    "def pick_value_col(df):\n",
    "    # prefer semantic names\n",
    "    for pat in (r\"^percent$\", r\"percent\", r\"\\bpct\\b\", r\"rate\", r\"per_?1000\"):\n",
    "        cols = [c for c in df.columns if re.search(pat, c, re.I)]\n",
    "        cols = [c for c in cols if c.lower() not in (\"shape_area\",\"shape_length\",\"objectid\")]\n",
    "        if cols:\n",
    "            return cols[0]\n",
    "    # fallback: numeric column with most non-null values (excluding id-ish)\n",
    "    num_cols = [c for c in df.columns\n",
    "                if c.lower() not in (\"objectid\",\"shape_area\",\"shape_length\") and\n",
    "                   pd.to_numeric(df[c], errors=\"coerce\").notna().sum() > 0]\n",
    "    if not num_cols:\n",
    "        raise ValueError(\"No numeric value column found.\")\n",
    "    return num_cols[0]\n",
    "\n",
    "CSA_KEY  = pick_csa_key(csa)\n",
    "VALUE_COL= pick_value_col(csa)\n",
    "print(\"CSA key:\", CSA_KEY, \"| value column:\", VALUE_COL)\n",
    "\n",
    "# 4) harmonize types and clean numbers\n",
    "csa = csa.rename(columns={CSA_KEY: \"csa_id\"})\n",
    "csa[\"csa_id\"] = csa[\"csa_id\"].astype(str).str.strip()\n",
    "cw[\"csa_id\"]  = cw[\"csa_id\"].astype(str).str.strip()\n",
    "\n",
    "csa[VALUE_COL] = pd.to_numeric(\n",
    "    csa[VALUE_COL].astype(str).str.replace(\",\", \"\", regex=False).str.replace(\"%\",\"\", regex=False),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# 5) merge + area-weight downscale\n",
    "m = csa.merge(cw, on=\"csa_id\", how=\"inner\")\n",
    "m[\"weighted_value\"] = m[VALUE_COL] * m[\"weight_area\"]\n",
    "\n",
    "tract_rate = (m.groupby(\"tract_id\", as_index=False)[\"weighted_value\"]\n",
    "                .sum()\n",
    "                .rename(columns={\"weighted_value\": \"pct_hs_or_more_areaweighted\"}))\n",
    "\n",
    "save_parquet(tract_rate, \"tract_pct_hs_or_more.parquet\")\n",
    "tract_rate.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99be3c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ../data_raw/community/Number_of_Banks_and_Bank_Branches_per_1,000_Residents_-_Community_Statistical_Area.csv\n",
      "CSA key: CSA2010 | value column: banks11\n",
      "saved → ../data_clean/community/tract_banks_per_1000.parquet | rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tract_id</th>\n",
       "      <th>banks_per_1000_areaweighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tract_id, banks_per_1000_areaweighted]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 (replace): Downscale a per-1,000 CSA metric to tracts (area-weighted)\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "COMM_DIR = ROOT / \"data_raw\" / \"community\"\n",
    "pattern = re.compile(r\"Banks?_and_Bank_Branches.*1,?000\", re.I)\n",
    "candidates = [p.name for p in COMM_DIR.glob(\"*.csv\")]\n",
    "match = next((f for f in candidates if pattern.search(f)), None)\n",
    "if not match:\n",
    "    raise FileNotFoundError(\"Could not find the Banks per 1,000 file. Pick one from:\\n\" + \"\\n\".join(candidates))\n",
    "\n",
    "INPUT_CSV = COMM_DIR / match\n",
    "print(\"Using:\", INPUT_CSV)\n",
    "\n",
    "csa = pd.read_csv(INPUT_CSV)\n",
    "csa.columns = [c.strip() for c in csa.columns]\n",
    "\n",
    "def pick_csa_key(df):\n",
    "    for k in (\"CSA2010\",\"CSA_NAME\",\"NAME\",\"CSA_ID\"):\n",
    "        if k in df.columns:\n",
    "            return k\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object and c.lower() not in (\"shape_area\",\"shape_length\",\"geometry\",\"objectid\"):\n",
    "            return c\n",
    "    raise ValueError(\"No CSA key found.\")\n",
    "\n",
    "def pick_value_col(df):\n",
    "    # look for per-1000-ish column names\n",
    "    for pat in (r\"per.?1,?000\", r\"per_?1000\", r\"rate\"):\n",
    "        cols = [c for c in df.columns if re.search(pat, c, re.I)]\n",
    "        cols = [c for c in cols if c.lower() not in (\"shape_area\",\"shape_length\",\"objectid\")]\n",
    "        if cols:\n",
    "            return cols[0]\n",
    "    # fallback to most numeric column excluding ids\n",
    "    num_cols = [c for c in df.columns\n",
    "                if c.lower() not in (\"objectid\",\"shape_area\",\"shape_length\") and\n",
    "                   pd.to_numeric(df[c], errors=\"coerce\").notna().sum() > 0]\n",
    "    if not num_cols:\n",
    "        raise ValueError(\"No numeric value column found.\")\n",
    "    return num_cols[0]\n",
    "\n",
    "CSA_KEY  = pick_csa_key(csa)\n",
    "VALUE_COL= pick_value_col(csa)\n",
    "print(\"CSA key:\", CSA_KEY, \"| value column:\", VALUE_COL)\n",
    "\n",
    "csa = csa.rename(columns={CSA_KEY: \"csa_id\"})\n",
    "csa[\"csa_id\"] = csa[\"csa_id\"].astype(str).str.strip()\n",
    "cw[\"csa_id\"]  = cw[\"csa_id\"].astype(str).str.strip()\n",
    "\n",
    "csa[VALUE_COL] = pd.to_numeric(\n",
    "    csa[VALUE_COL].astype(str).str.replace(\",\", \"\", regex=False),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "m = csa.merge(cw, on=\"csa_id\", how=\"inner\")\n",
    "m[\"weighted_value\"] = m[VALUE_COL] * m[\"weight_area\"]\n",
    "\n",
    "tract_banks = (m.groupby(\"tract_id\", as_index=False)[\"weighted_value\"]\n",
    "                 .sum()\n",
    "                 .rename(columns={\"weighted_value\": \"banks_per_1000_areaweighted\"}))\n",
    "\n",
    "save_parquet(tract_banks, \"tract_banks_per_1000.parquet\")\n",
    "tract_banks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9796db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "TRACTS_FP = ROOT / \"shapes\" / \"baltimore_tracts_2020.geojson\"\n",
    "tracts = gpd.read_file(TRACTS_FP)\n",
    "tracts = tracts[[\"tract_id\",\"geometry\"]]\n",
    "\n",
    "def load_points_any(path: Path, lon_col=None, lat_col=None, crs_epsg=4326):\n",
    "    \"\"\"\n",
    "    Load a point layer from CSV/GeoJSON/Shapefile.\n",
    "    If CSV, you must pass lon_col/lat_col. Returns a GeoDataFrame in tract CRS.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        assert lon_col and lat_col, \"For CSV points, provide lon_col and lat_col.\"\n",
    "        df = pd.read_csv(path)\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(df[lon_col], df[lat_col], crs=f\"EPSG:{crs_epsg}\")\n",
    "        )\n",
    "    else:\n",
    "        gdf = gpd.read_file(path)\n",
    "        if gdf.crs is None:\n",
    "            gdf.set_crs(epsg=crs_epsg, inplace=True)\n",
    "    # project to match tracts\n",
    "    return gdf.to_crs(tracts.crs)\n",
    "\n",
    "def points_to_tract_counts(points_gdf, id_col=\"tract_id\", how=\"within\"):\n",
    "    \"\"\"Spatial join → counts per tract.\"\"\"\n",
    "    joined = gpd.sjoin(points_gdf, tracts, how=\"left\", predicate=how)\n",
    "    counts = (joined.groupby(id_col).size()\n",
    "                    .reset_index(name=\"count\")\n",
    "                    .rename(columns={id_col:\"tract_id\"}))\n",
    "    return counts\n",
    "\n",
    "def nearest_distance_to_feature(points_gdf, k=1):\n",
    "    \"\"\"\n",
    "    For each tract polygon centroid, compute distance to nearest point (in meters).\n",
    "    Returns DataFrame tract_id, dist_nearest_m.\n",
    "    \"\"\"\n",
    "    # work in a metric CRS\n",
    "    proj = tracts.estimate_utm_crs()\n",
    "    tr_p = tracts.to_crs(proj).copy()\n",
    "    pts_p = points_gdf.to_crs(proj).copy()\n",
    "\n",
    "    # build spatial index\n",
    "    sindex = pts_p.sindex\n",
    "\n",
    "    def nearest_dist(geom):\n",
    "        cand_idx = list(sindex.nearest(geom.bounds, num_results=k))\n",
    "        return float(geom.distance(pts_p.iloc[cand_idx[0]].geometry))\n",
    "\n",
    "    tr_p[\"dist_nearest_m\"] = tr_p.geometry.centroid.apply(nearest_dist)\n",
    "    return tr_p[[\"tract_id\",\"dist_nearest_m\"]].copy()\n",
    "\n",
    "def save_feat(df: pd.DataFrame, name: str):\n",
    "    out = CLEAN_COMM / name\n",
    "    df.to_parquet(out, index=False)\n",
    "    print(\"saved →\", out, \"| rows:\", len(df))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a01233d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8 — utilities for converting point layers into tract-level features\n",
    "# ============================================================\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "TRACTS_FP = ROOT / \"shapes\" / \"baltimore_tracts_2020.geojson\"\n",
    "tracts = gpd.read_file(TRACTS_FP)[[\"tract_id\", \"geometry\"]]\n",
    "\n",
    "def load_points_any(path: Path, lon_col=None, lat_col=None, crs_epsg=4326):\n",
    "    \"\"\"\n",
    "    Load a point layer from CSV/GeoJSON/Shapefile.\n",
    "    • Auto-detects longitude/latitude field names if not given.\n",
    "    • Returns a GeoDataFrame projected to match the tracts layer.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # --- Auto-detect lon/lat fields ---\n",
    "        def pick(cols, cands):\n",
    "            for c in cols:\n",
    "                cl = c.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "                if cl in cands:\n",
    "                    return c\n",
    "            return None\n",
    "\n",
    "        cols = list(df.columns)\n",
    "\n",
    "        if not lon_col or not lat_col:\n",
    "            lon_candidates = {\n",
    "                \"longitude\",\"long\",\"lon\",\"x\",\"pointx\",\"xcoord\",\"xcoordinate\",\n",
    "                \"locationlongitude\",\"lng\"\n",
    "            }\n",
    "            lat_candidates = {\n",
    "                \"latitude\",\"lat\",\"y\",\"pointy\",\"ycoord\",\"ycoordinate\",\n",
    "                \"locationlatitude\"\n",
    "            }\n",
    "            lon_col = lon_col or pick(cols, lon_candidates)\n",
    "            lat_col = lat_col or pick(cols, lat_candidates)\n",
    "\n",
    "        if lon_col is None or lat_col is None:\n",
    "            raise KeyError(\n",
    "                f\"Couldn't find lon/lat columns. Columns are: {cols}. \"\n",
    "                f\"Pass lon_col=..., lat_col=... explicitly.\"\n",
    "            )\n",
    "\n",
    "        # --- build geometry ---\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(\n",
    "                pd.to_numeric(df[lon_col], errors=\"coerce\"),\n",
    "                pd.to_numeric(df[lat_col], errors=\"coerce\"),\n",
    "                crs=f\"EPSG:{crs_epsg}\"\n",
    "            )\n",
    "        ).dropna(subset=[\"geometry\"])\n",
    "\n",
    "    else:\n",
    "        # --- handle GeoJSON, Shapefile, etc. ---\n",
    "        gdf = gpd.read_file(path)\n",
    "        if gdf.crs is None:\n",
    "            gdf.set_crs(epsg=crs_epsg, inplace=True)\n",
    "\n",
    "    return gdf.to_crs(tracts.crs)\n",
    "\n",
    "def points_to_tract_counts(points_gdf, id_col=\"tract_id\", how=\"within\"):\n",
    "    \"\"\"Spatial join points to tracts and count points per tract.\"\"\"\n",
    "    joined = gpd.sjoin(points_gdf, tracts, how=\"left\", predicate=how)\n",
    "    counts = (joined.groupby(id_col)\n",
    "                    .size()\n",
    "                    .reset_index(name=\"count\")\n",
    "                    .rename(columns={id_col: \"tract_id\"}))\n",
    "    return counts\n",
    "\n",
    "def nearest_distance_to_feature(points_gdf, k=1):\n",
    "    \"\"\"\n",
    "    For each tract centroid, compute distance to nearest point (in meters).\n",
    "    Returns DataFrame [tract_id, dist_nearest_m].\n",
    "    \"\"\"\n",
    "    proj = tracts.estimate_utm_crs()\n",
    "    tr_p = tracts.to_crs(proj).copy()\n",
    "    pts_p = points_gdf.to_crs(proj).copy()\n",
    "    sindex = pts_p.sindex\n",
    "\n",
    "    def nearest_dist(geom):\n",
    "        cand_idx = list(sindex.nearest(geom.bounds, num_results=k))\n",
    "        return float(geom.distance(pts_p.iloc[cand_idx[0]].geometry))\n",
    "\n",
    "    tr_p[\"dist_nearest_m\"] = tr_p.geometry.centroid.apply(nearest_dist)\n",
    "    return tr_p[[\"tract_id\", \"dist_nearest_m\"]].copy()\n",
    "\n",
    "def save_feat(df: pd.DataFrame, name: str):\n",
    "    out = CLEAN_COMM / name\n",
    "    df.to_parquet(out, index=False)\n",
    "    print(\"saved →\", out, \"| rows:\", len(df))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9d208a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grocery file columns: ['X', 'Y', 'OBJECTID', 'latitude', 'longitude', 'address', 'storename', 'in_oz', 'label', 'renovationincentiveelig', 'status', 'oz_sc', 'congressionaldistrict_md', 'congressionaldistrict_usno', 'md_ld', 'GlobalID']\n"
     ]
    }
   ],
   "source": [
    "# quick check — see what columns the grocery CSV actually has\n",
    "grocery_fp = ROOT / \"data_raw\" / \"community\" / \"Grocery_Stores.csv\"  # adjust if the filename differs\n",
    "cols = pd.read_csv(grocery_fp, nrows=2).columns.tolist()\n",
    "print(\"Grocery file columns:\", cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d1be8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved → ../data_clean/community/tract_groceries_count.parquet | rows: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('../data_clean/community/tract_groceries_count.parquet')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grocery = load_points_any(grocery_fp)  # auto-detect will work\n",
    "gr_counts = points_to_tract_counts(grocery)\n",
    "save_feat(gr_counts, \"tract_groceries_count.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6aa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helpers & setup (safe to rerun) ===\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT        = Path(\"..\")  # notebook lives in /notebooks\n",
    "CLEAN_COMM  = ROOT / \"data_clean\" / \"community\"\n",
    "CLEAN_COMM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRACTS_FP = ROOT / \"shapes\" / \"baltimore_tracts_2020.geojson\"\n",
    "tracts = gpd.read_file(TRACTS_FP)[[\"tract_id\", \"geometry\"]]\n",
    "\n",
    "def load_points_any(path: Path, lon_col=None, lat_col=None, crs_epsg=4326):\n",
    "    \"\"\"\n",
    "    Load a point layer from CSV/GeoJSON/Shapefile.\n",
    "    Auto-detects lon/lat names if not provided. Returns GeoDataFrame in tract CRS.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        def pick(cols, cands):\n",
    "            for c in cols:\n",
    "                cl = c.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "                if cl in cands: return c\n",
    "            return None\n",
    "\n",
    "        cols = list(df.columns)\n",
    "        if not lon_col or not lat_col:\n",
    "            lon_col = lon_col or pick(cols, {\n",
    "                \"longitude\",\"long\",\"lon\",\"x\",\"pointx\",\"xcoord\",\"xcoordinate\",\"locationlongitude\",\"lng\"\n",
    "            })\n",
    "            lat_col = lat_col or pick(cols, {\n",
    "                \"latitude\",\"lat\",\"y\",\"pointy\",\"ycoord\",\"ycoordinate\",\"locationlatitude\"\n",
    "            })\n",
    "        if lon_col is None or lat_col is None:\n",
    "            raise KeyError(f\"Couldn't find lon/lat. Columns: {cols}. Pass lon_col=..., lat_col=...\")\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(\n",
    "                pd.to_numeric(df[lon_col], errors=\"coerce\"),\n",
    "                pd.to_numeric(df[lat_col], errors=\"coerce\"),\n",
    "                crs=f\"EPSG:{crs_epsg}\"\n",
    "            )\n",
    "        ).dropna(subset=[\"geometry\"])\n",
    "    else:\n",
    "        gdf = gpd.read_file(path)\n",
    "        if gdf.crs is None:\n",
    "            gdf.set_crs(epsg=crs_epsg, inplace=True)\n",
    "\n",
    "    return gdf.to_crs(tracts.crs)\n",
    "\n",
    "def points_to_tract_counts(points_gdf, id_col=\"tract_id\", how=\"within\"):\n",
    "    joined = gpd.sjoin(points_gdf, tracts, how=\"left\", predicate=how)\n",
    "    return (joined.groupby(id_col).size()\n",
    "                  .reset_index(name=\"count\")\n",
    "                  .rename(columns={id_col: \"tract_id\"}))\n",
    "\n",
    "def nearest_distance_to_feature(points_gdf, k=1):\n",
    "    proj = tracts.estimate_utm_crs()\n",
    "    tr_p = tracts.to_crs(proj).copy()\n",
    "    pts_p = points_gdf.to_crs(proj).copy()\n",
    "    sindex = pts_p.sindex\n",
    "    def nearest_dist(geom):\n",
    "        idx = list(sindex.nearest(geom.bounds, num_results=k))\n",
    "        return float(geom.distance(pts_p.iloc[idx[0]].geometry))\n",
    "    tr_p[\"dist_nearest_m\"] = tr_p.geometry.centroid.apply(nearest_dist)\n",
    "    return tr_p[[\"tract_id\",\"dist_nearest_m\"]]\n",
    "\n",
    "def save_feat(df: pd.DataFrame, name: str):\n",
    "    out = CLEAN_COMM / name\n",
    "    df.to_parquet(out, index=False)\n",
    "    print(\"saved →\", out, \"| rows:\", len(df))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c991aaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/warrenjones/Dev/igs-analysis-baltimore/notebooks/data_intermediate/tract_schools_count.parquet.parquet'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load your school points ---\n",
    "schools_fp = ROOT / \"data_raw\" / \"shapes\" / \"Baltimore_City_Schools.geojson\"\n",
    "schools = load_points_any(schools_fp)  # GeoJSON already has geometry\n",
    "\n",
    "# --- Load the standardized Baltimore tracts from your shapes folder ---\n",
    "tracts_fp = ROOT / \"shapes\" / \"baltimore_tracts_2020.geojson\"\n",
    "tracts = load_polygons_any(tracts_fp)  # function reprojects for spatial ops\n",
    "\n",
    "# --- Compute counts ---\n",
    "sch_counts = points_to_tract_counts(schools, tracts, tract_id_col=\"tract_id\", count_col=\"schools_count\")\n",
    "\n",
    "# --- Save results ---\n",
    "save_feat(sch_counts, \"tract_schools_count.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "572bd35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schools: EPSG:4326 | tracts: EPSG:3857\n",
      "points total: 155  | joined: 155  | coverage: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# 1) CRS sanity\n",
    "print(\"schools:\", schools.crs, \"| tracts:\", tracts.crs)\n",
    "\n",
    "# 2) join coverage — % of points assigned to a tract\n",
    "pts_total = len(schools)\n",
    "pts_joined = sch_counts[\"schools_count\"].sum()\n",
    "print(\"points total:\", pts_total, \" | joined:\", pts_joined, \" | coverage:\", round(100*pts_joined/pts_total, 2), \"%\")\n",
    "\n",
    "# 3) tract_id integrity\n",
    "assert sch_counts[\"tract_id\"].notna().all()\n",
    "assert sch_counts[\"tract_id\"].is_unique\n",
    "\n",
    "# 4) counts are nonnegative integers\n",
    "assert (sch_counts[\"schools_count\"] >= 0).all()\n",
    "assert (sch_counts[\"schools_count\"].dropna().astype(int) == sch_counts[\"schools_count\"]).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "618043af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"..\") if Path.cwd().name == \"notebooks\" else Path(\".\")\n",
    "RAW_SHAPES  = ROOT / \"data_raw\" / \"shapes\"\n",
    "PROJ_SHAPES = ROOT / \"shapes\"\n",
    "TRACTS_RAW  = RAW_SHAPES / \"2020_Census_Tracts_(Census_TIGER).geojson\"\n",
    "CSAS_RAW    = RAW_SHAPES / \"Community_Statistical_Areas_(CSAs)__Reference_Boundaries.geojson\"\n",
    "TRACTS_FP   = PROJ_SHAPES / \"baltimore_tracts_2020.geojson\"\n",
    "CSAS_FP     = PROJ_SHAPES / \"baltimore_csa.geojson\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
